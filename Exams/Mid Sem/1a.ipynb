import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d

def f(x, y):
   return (x ** 2) + (y ** 2) + (2 * x) + (2 * y)

x1 = np.linspace(-10, 10, 100)
x2 = np.linspace(-10, 10, 100)

X, Y = np.meshgrid(x1, x2)
Z = f(X, Y)

fig = plt.figure()
ax = plt.axes(projection='3d')
ax.plot_surface(X, Y, Z, cmap='viridis')
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('f1(x1, x2)')
ax.set_title('Surface Curve for f1')
plt.show()

plt.figure()
plt.contour(X, Y, Z, 30)
plt.scatter(-1, -1, c='b', marker='.')
plt.show()

# -------------------------------------------------------------------

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d

def f(x, y):
   return (x * np.sin(x)) + (y * np.sin(y))
	
x1 = np.linspace(-10, 10, 100)
x2 = np.linspace(-10, 10, 100)

X, Y = np.meshgrid(x1, x2)
Z = f(X, Y)

fig = plt.figure()
ax = plt.axes(projection='3d')
ax.plot_surface(X, Y, Z, cmap='viridis')
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('f2(x1, x2)')
ax.set_title('Surface Curve for f2')
plt.show()

plt.figure()
plt.contour(X, Y, Z, 30)
plt.xlim(-11, 11)
plt.ylim(-11, 11)
plt.scatter(10, 10, c='b', marker='.')
plt.scatter(-10, 10, c='b', marker='.')
plt.scatter(-10, -10, c='b', marker='.')
plt.scatter(10, -10, c='b', marker='.')
plt.show()

#------------------------------------------------------------------------

# Gradient Descent Function
def gradient_descent(x1, x2, grad_func, no_iterations, learning_rate):
  for i in range(no_iterations):
    if (x1 >= -10 or x2 >= -10):
      x1 = x1 - grad_func(x1)*learning_rate
      x2 = x2 - grad_func(x2)*learning_rate
    else:
      break
  print(x1, x2)
  
#------------------------------------------------------------------------

def derivative_f1(x):
  return 2*x + 2
def derivative_f2(x):
  return x*np.cos(x) + np.sin(x)
  
#------------------------------------------------------------------------

print("x1 & x2: ")
gradient_descent(1, 2, derivative_f1, 1000, 0.1)
print("x1 & x2: ")
gradient_descent(-8, -8, derivative_f2, 1000, 0.01)