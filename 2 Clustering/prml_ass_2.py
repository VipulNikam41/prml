# -*- coding: utf-8 -*-
"""prml-ass-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10v0FTiNJK8IQs0yJhUVuFqukgpQ-qXvD

Clustering:
1. K-means
2. Fuzzy C-means
3. GMM
4. Practical Example (repeat for all 3)
"""

import matplotlib.pyplot as plt

def plot_points(points, centers=None, markers=['^','^','s','s','^','s'], verbose=False):
  fig, ax = plt.subplots()

  if verbose:
    print(np.array(points).shape, np.array(centers).shape, ' passed ')

  for (xp, yp), m in zip(points, markers):
      ax.scatter(xp, yp, marker=m)

  if centers is not None:
    for (x, y) in centers:
      ax.scatter(x, y, marker='o', color='black')


  plt.show()

"""# 1. K-means clustering 
a) Data genearation

b) Generate 2D gaussian data of 4 types each having 100 points, by taking appropriate mean and varince (example: mean :(0.5 0) (5 5) (5 1) (10 1.5), variance : Identity matrix)
"""

import numpy as np
import matplotlib.pyplot as plt

## Data generation
variance = np.identity(2)
means = [(0.5, 0), (5, 5), (5, 1), (10, 1.5)]
# points = [np.random.multivariate_normal(mean, variance, 100).T for mean in means]
points = np.random.rand(4, 2, 100) * 20

plot_points(points, verbose=True)

"""## Cluster Initialization

a) Randomly initialize the cluster centers (any k- number of data points from the generated data)
"""

centers = np.random.rand(4, 2) * 20
print(centers)
plot_points(points=points, centers=centers, verbose=True)

"""## Cluster assignment and re-estimation Stage 

a) Using initial/estimated cluster centers (mean $\mu_{i}$) perform cluster assignment.

b) Assigned cluster for each feature vector ($X_{j}$) can be written as:
$$arg \min_{i} ||C_{i}-X_{j}||_{2},~1 \leq i \leq K,~1\leq j \leq N$$ 
c) Re-estimation: After cluster assignment, the mean vector is recomputed as,
$$\mu_{i}=\frac{1}{N_{i}}\sum_{j \in i^{th} cluster}X_{j}$$
where $N_{i}$ represents the number of datapoints in the $i^{th}$ cluster.

d) Objective function (to be minimized):
$$Error(\mu)=\frac{1}{N}\sum_{i=1}^{K}\sum_{j \in i^{th} cluster}||C_{i}-X_{j}||_{2}$$
"""

errors = []

def dist(p1, p2):
  return ( (p1[0]-p2[0])**2 + (p1[1]-p2[1])**2 )**0.5

def update_cluster(pt, centers):
  # updates cluster val of given point
  distances = [dist(pt[:2], center) for center in centers]
  return pt[0], pt[1], np.array(distances).argmin()

def mean(points):
  N = 1 if len(points) == 0 or len(points[0]) == 0 else len(points[0])
  return sum(x for (x, *y) in zip(*points))/ N, sum(y for (*x, y) in zip(*points))/ N

def error(points, centers):
  agg = 0
  for i in range(4):
    for pt in zip(*points[i]):
      agg += dist(pt, centers[i])

  return agg / len(points)

centers = list(centers)
points = list(points)
num_epoch = 10

for epoch in range(num_epoch):

  # update cluster of each point
  new_points = [[[], []] for _ in range(4)]
  for index, cluster in enumerate(points):

    for (x, y) in zip(*cluster):
      new_point = update_cluster((x, y), centers)
      index = new_point[-1]
      new_points[index][0].append(new_point[0])
      new_points[index][1].append(new_point[1])


  points = new_points

  # update centers
  for i in range(4):
    centers[i] = mean(points[i])

  errors.append(error(points, centers))

  plot_points(points, centers, verbose=True)

plt.plot(errors)
plt.show()

"""# 2. GMM Clustering

##1.  Data generation

a) Use the same data that you generated for K-means
"""

import numpy as np
import matplotlib.pyplot as plt

## Data generation
# write your code here

"""## 2. Initialization

a) Mean vector (randomly any from the given data points) ($\mu_{k}$)

b) Coveriance (initialize with (identity matrix)*max(data))  ($\Sigma_{k}$)

c) Weights (uniformly) ($w_{k}$), 
with constraint: $\sum_{k=1}^{K}w_{k}=1$
"""

#%% Initialisations

def initialization(data,K):

  # write your code here
  return theta

"""##3. Expectation stage
$$\gamma_{ik}=\frac{w_{k}P(x_{i}|\Phi_{k})}{\sum_{k=1}^{K}w_{k}P(x_{i}|\Phi_{k})}$$

where,
$$\Phi_{k}=\{\mu_{k},\Sigma_{k}\}$$
$$\theta_{k}=\{\Phi_{k},w_{k}\}$$
$$w_{k}=\frac{N_{k}}{N}$$
$$N_{k}=\sum_{i=1}^{N}\gamma_{ik}$$
$$P(x_{i}|\Phi_{k})=\frac{1}{(2 \pi)^{d/2}|\Sigma_{k}|^{1/2}}e^{-(x_{i}-\mu_{k})^{T}\Sigma_{k}^{-1}(x_{i}-\mu_{k})}$$
"""

# Expectation stage

#%% E-Step GMM
from scipy.stats import multivariate_normal
def E_Step_GMM(data,K,theta):

    # write your code here

    return responsibility

"""## 3. Maximization stage
a) $w_{k}=\frac{N_{k}}{N}$, where  $N_{k}=\sum_{i=1}^{N}\gamma_{ik}$

b) $\mu_{k}=\frac{\sum_{i=1}^{N}\gamma_{ik}x_{i}}{N_{k}}$

c) $\Sigma_{k}=\frac{\sum_{i=1}^{N}\gamma_{ik}(x_{i}-\mu_{k})(x_{i}-\mu_{k})^{T}}{N_{k}}$

Objective function(maximized through iteration):
$$L(\theta)=\sum_{i=1}^{N}log\sum_{k=1}^{K}w_{k}P(x_{i}|\Phi_{k})$$
"""

# Maximization stage

#%% M-STEP GMM
def M_Step_GMM(data,responsibility):
    # write your code here
            
    return theta, log_likelihood

"""## 4. Final run (EM algorithem)
a) initialization

b)Itterate E-M untill $L(\theta_{n})-L(\theta_{n-1}) \leq th$ 

c) Plot and see the cluster allocation at each itteration
"""

log_l=[]
Itr=50
eps=10**(-14)  # for threshold
clr=['r','g','b','y','k','m','c']
mrk=['+','*','X','o','.','<','p']


K=4   # no. of clusters

theta=initialization(data,K)

for n in range(Itr):

  responsibility=E_Step_GMM(data,K,theta)

  cluster_label=np.argmax(responsibility,axis=1) #Label Points

  theta,log_likhd=M_Step_GMM(data,responsibility)

  log_l.append(log_likhd)

  plt.figure()
  for l in range(K):
    id=np.where(cluster_label==l)
    plt.plot(data[id,0],data[id,1],'.',color=clr[l],marker=mrk[l])
  Cents=theta[0].T
  plt.plot(Cents[:,0],Cents[:,1],'X',color='k')
  plt.title('Iteration= %d' % (n))

  if n>2:
    if abs(log_l[n]-log_l[n-1])<eps:
      break


plt.figure()  
plt.plot(log_l)

"""# 3. Write a code and report similar demonstration for Fuzzy c-means



(Note : Generate the data such that you can demonstare the drawback of K-means, and able to solve through GMM and fuzzy C-means, have to demonstrate clearly during viva)

# 4. Practical Example

## Using K-means

a) Data preparation




1. Load Mnist data
2. Take only two class '1' and '5'
"""

from google.colab import drive
drive.mount('/gdrive')
!pip install idx2numpy

import numpy as np
import matplotlib.pyplot as plt 

# write you code here

"""2. Write a function of Kmeans as written earlier"""

# k-means

def K_means_clustering(data,K,itr,eps):
  # random initialization of clusters
# write your code here


  for n in range(itr):
# assignment stage
   # write your code here

# re-estimation stage
   for i in range(K):
      # write your code here

      

   error.append(np.mean(DAL[:,K+1]))  
   #print(Cents)



   if n>2:
      if abs(error[n]-error[n-1])<eps:
         break

   print(n)
   
  return DAL, Cents,error

"""3. Call the K-means function and plot the mean vectors of the cluster"""

DAL,cents,error=K_means_clustering(data,2,200,10**(-20))

plt.imshow(np.reshape(cents[0,:],(28,28)))
plt.figure()
plt.imshow(np.reshape(cents[1,:],(28,28)))

plt.figure()
plt.plot(error)

"""# 5. Perform the same task for GMM and fuzzy c-means

# 6. Repeat the same for 3 class and perform the K-means, GMM and Fuzzy c-means clustering

# 7. Perform DBSCAN and show the advantages of DBSCAN over model and distance based clustering. 

expected: (should visualize the cluster pattern that Model and distance based clustering can not able to capture but can be captured through DBSCAN)

# 8. Hierarchical Clustering

Hierarchical clustering is an unsupervised clustering technique which groups together the unlabelled data of similar characteristics.

There are two types of hierarchical clustering:


*   Agglomerative Clustering
*   Divisive Clustering

**Agglomerative Clustering:** 

 In this type of hierarchical clustering all data set are considered as indivisual cluster and at every iterations clusters with similar characteristics are merged to give bigger clusters. This is repeated untill one single cluster is reached. It is also called bottem-top approach.


**Divisive Clustering:**

It is an opposite of Agglomerative clustering. In this we start from one cluster which contains all data points in one.  Iteratively we separate all the cluster of points which aren't similar in characteristics. It is also called top-bottom approach.

## Agglomerative Clustering:

Lets start with some domy example :

X=$[x_1,x_2,...,x_5]$, with

$x_1=\begin{bmatrix} 1\\1\end{bmatrix}$, $x_2=\begin{bmatrix} 2\\1\end{bmatrix}$, $x_3=\begin{bmatrix} 5\\4\end{bmatrix}$, $x_4=\begin{bmatrix} 6\\5\end{bmatrix}$, $x_5=\begin{bmatrix} 6.5\\6\end{bmatrix}$

**Steps to perform Agglomerative Clustering:**





1.   Compute Distance matrix ($N \times N$ matrix, where $N$ number of vectors present in the dataset):
$D(a,b)=||x_{a}-x_{b}||_{2}$
2.   Replace the diagonal elements with $inf$ and find the index of the minimum element present in the distance matrix (suppose we get the location $(l,k)$).
3. Replace $x_{min(l,k)}=.5\times[x_{l}+x_{m}]$ and delete $x_{max(l,m)}$ vector from $X$(i.e now $(N=N-1)$), 

**repeat from step 1 again untill all the vectors combined to a single cluster.**
"""

import numpy as np
def Euclidian_Dist(x,y):
  
  return #write your code here

def Dist_mat(X):
 #write your code here
  return dist_mat

def combine(X):
  #write your code here

  return newX

X=np.array([[1,1],[2,1],[5,4],[6,5],[6.5,6]])
X=X.transpose()

#write your code here
## velidate from inbuilt Dendogram

import plotly.figure_factory as ff



lab=np.linspace(1,X.shape[1],X.shape[1])
fig = ff.create_dendrogram(X.T, labels=lab)
fig.update_layout(width=800, height=300)
fig.show()

"""## Divisive clustering:

It is a top down approach of hierarchial clustering

Lets start with some domy example :

X=$[x_1,x_2,...,x_5]$, with

$x_1=\begin{bmatrix} 1\\1\end{bmatrix}$, $x_2=\begin{bmatrix} 2\\1\end{bmatrix}$, $x_3=\begin{bmatrix} 5\\4\end{bmatrix}$, $x_4=\begin{bmatrix} 6\\5\end{bmatrix}$, $x_5=\begin{bmatrix} 6.5\\6\end{bmatrix}$



1.   Find the biggest cluster (having highest diameter), initially the single cluster is the biggest cluster.

$$Diameter_{cluster}=\max_{i,j}||x_{i}-x_{j}||_{2}$$

$i,j$ will move over all the elements in the cluster.

2.  find the splinter element of the cluster by using the maximum average distance between the other elements. 
$$d_{k}=\frac{1}{N-1}\sum_{i=1}^{N}||x_{k}-x_{i}||_{2}$$
$splinter-group-element=arg \max_{1 \leq k \leq N} (d_{k})$

repeat the same and assign element to the splinter group untill the differance between average incluster distance and average splinter group distance of each element turns negative.

$$d_{avgsplint_{k}}=\frac{1}{M-1}\sum_{i=1}^{M}||x_{k}-x_{i}||_{2}$$

Stop: $$d_{k}-d_{avgsplint_{k}}<0$$
 and assign the splinter group as a new cluster.

 3. Repeat the step 1 and 2 untill each cluster have only one element.

 4.**Plot the cluster split with respect to their diameter**
"""

import numpy as np
def Euclidian_Dist(x,y):
  return #write your code here

def Dist_mat(X):
  #write your code here
  return dist_mat


def avg_distance(X):
  #write your code here
  return #write your code here

def get_diameter(X, i):
    """Returns the diameter of the ith cluster in X"""
    #write your code here
    return diameter

def get_biggest_cluster(X):
    """ Returns the cluster index having largest diameter"""
    #write your code here 
    # index having max diameter
    return max_cluster_ind

def avg_spl_dists(cluster, splinter):
    """ Return the average of distances of each point belonging to cl wrt splinter"""
    #write your code here
    return avg_dists

# Implement Divisive Clustering 
import numpy as np
X = np.array([[1,1], [2,1], [5,4], [6,5], [6.5,6]])
X = X.transpose() # Shape after transpose: [2, 5]
num_points = X.shape[1]
print(f'X:\n {X}')

#write your code here

"""# 9.Take a real data example and demonstrate both Agglomerative and Divisive clustering."""